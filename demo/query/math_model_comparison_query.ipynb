{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Model Comparison Query and Query Results Approximation in Task-Me-Anything\n",
    "\n",
    "In this notebook, we will show how to perform a “Model Comparison Query” in Task-Me-Anything. We’ll compare the performance of `llavav1.5-7b` with the baseline model `instructblip-flant5xl` over 3200+ task plans on “2D sticker how many” task type, by finding the task plan that performance  of `llavav1.5-7b` is significant higher than `instructblip-flant5xl`. After that, we willl using `Fit` and `Active` query results approximation algorithms to approximate the performance of tasks plan within only 500 budgets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate tasks\n",
    "\n",
    "These are the process of task plans generation, illustrations on these part will be in the `generate` part of demo.\n",
    "\n",
    "In this step, we generate 3,249 “how many” task plans in 2D scenarios. Each task plan contains all the configuration and content needed to generate an image-question pair (test instance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enumerating templates with 1 params: 100%|██████████| 100/100 [00:00<00:00, 269556.81it/s]\n",
      "Enumerating templates with 1 params: 100%|██████████| 100/100 [00:00<00:00, 702563.48it/s]\n",
      "Enumerating templates with 1 params: 100%|██████████| 100/100 [00:00<00:00, 747647.77it/s]\n",
      "Enumerating templates with 1 params: 100%|██████████| 100/100 [00:00<00:00, 721911.19it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_template</th>\n",
       "      <th>radius</th>\n",
       "      <th>circumference</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the perimeter (circumference) of a cir...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.283185307179586</td>\n",
       "      <td>3.141592653589793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the perimeter (circumference) of a cir...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.566370614359172</td>\n",
       "      <td>12.566370614359172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the perimeter (circumference) of a cir...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18.84955592153876</td>\n",
       "      <td>28.274333882308138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the perimeter (circumference) of a cir...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>25.132741228718345</td>\n",
       "      <td>50.26548245743669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the perimeter (circumference) of a cir...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>31.41592653589793</td>\n",
       "      <td>78.53981633974483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>What is the perimeter (circumference) of a cir...</td>\n",
       "      <td>96.0</td>\n",
       "      <td>603.1857894892403</td>\n",
       "      <td>28952.917895483533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>What is the perimeter (circumference) of a cir...</td>\n",
       "      <td>97.0</td>\n",
       "      <td>609.4689747964198</td>\n",
       "      <td>29559.245277626364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>What is the perimeter (circumference) of a cir...</td>\n",
       "      <td>98.0</td>\n",
       "      <td>615.7521601035994</td>\n",
       "      <td>30171.855845076374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>What is the perimeter (circumference) of a cir...</td>\n",
       "      <td>99.0</td>\n",
       "      <td>622.0353454107791</td>\n",
       "      <td>30790.74959783356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>What is the perimeter (circumference) of a cir...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>628.3185307179587</td>\n",
       "      <td>31415.926535897932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    question_template radius  \\\n",
       "0   What is the perimeter (circumference) of a cir...    1.0   \n",
       "1   What is the perimeter (circumference) of a cir...    2.0   \n",
       "2   What is the perimeter (circumference) of a cir...    3.0   \n",
       "3   What is the perimeter (circumference) of a cir...    4.0   \n",
       "4   What is the perimeter (circumference) of a cir...    5.0   \n",
       "..                                                ...    ...   \n",
       "95  What is the perimeter (circumference) of a cir...   96.0   \n",
       "96  What is the perimeter (circumference) of a cir...   97.0   \n",
       "97  What is the perimeter (circumference) of a cir...   98.0   \n",
       "98  What is the perimeter (circumference) of a cir...   99.0   \n",
       "99  What is the perimeter (circumference) of a cir...  100.0   \n",
       "\n",
       "         circumference                area  \n",
       "0    6.283185307179586   3.141592653589793  \n",
       "1   12.566370614359172  12.566370614359172  \n",
       "2    18.84955592153876  28.274333882308138  \n",
       "3   25.132741228718345   50.26548245743669  \n",
       "4    31.41592653589793   78.53981633974483  \n",
       "..                 ...                 ...  \n",
       "95   603.1857894892403  28952.917895483533  \n",
       "96   609.4689747964198  29559.245277626364  \n",
       "97   615.7521601035994  30171.855845076374  \n",
       "98   622.0353454107791   30790.74959783356  \n",
       "99   628.3185307179587  31415.926535897932  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "# set the working directory to the root of the project\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from tma.mathqa.math_metadata import MathTemplateMetaData\n",
    "from tma.task_store import TaskStore\n",
    "from tma.mathqa.geometry_task import CircleGenerator, AngleGenerator, IntersectionGenerator, MidpointGenerator, PerimeterGenerator\n",
    "\n",
    "template_path = \"../../annotations/math_annotations/circle_templates.json\"\n",
    "metadata = MathTemplateMetaData(template_path)\n",
    "generator = CircleGenerator(metadata)\n",
    "task_store = TaskStore(CircleGenerator.schema)\n",
    "generator.enumerate_task_plans(task_store)\n",
    "df = task_store.return_df()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding the tasks and create VQATaskEvaluator\n",
    "\n",
    "\n",
    "Task evaluator takes the model and the tasks as input, and evaluate and query the model's performance on the tasks generated by task plans. \n",
    "\n",
    "\n",
    "\n",
    "<!-- Because we want to fit a performance regressor, we need to embed the tasks. We will use the Cohere API to embed the tasks. First you need to set the `api_key` parameter to your Cohere API key. You can also using other embedding API or models to embed the tasks. (e.g Openai embedding API, BERT, etc.)\n",
    "\n",
    "Then you should create a `VQATaskEvaluator` object. `VQATaskEvaluator` is a class designed to evaluate a model's performance on task. It can handle the details in evaluate the model such as create the embedding of the tasks, fit the performance regressor, etc.\n",
    "\n",
    "Notice that `VQATaskEvaluator` can cache the embeddings to avoid redundant requests to the OpenAI API. You can change the path of the cache file by setting the `cache_path` parameter. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tma.task_evaluator import QATaskEvaluator\n",
    "\n",
    "task_evaluator = QATaskEvaluator(\n",
    "    task_plan_df=df, # data frames task plans to evaluate\n",
    "    task_generator=generator, # task generator, used to generate test instances for each task plan\n",
    "    embedding_name='st',  # using sentence transformer (st) to embedding questions\n",
    "    embedding_batch_size=10000,  # batch size for embedding\n",
    "    n_instance_per_task=5,  # number of test instances generated per task plan\n",
    "    n_trials_per_instance=3,  # number of trials per test instance\n",
    "    cache_path_root=\".cache\",  # enter your path for cache\n",
    "    seed=42  # random seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model on all the task plans\n",
    "\n",
    "In this steps, we will start to get the ground truth of the query. We will not use query approximation algorithms in this step. Instead, we will evaluate the model on all the tasks and get the top 10 worst-performing tasks as the ground truth. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can call tma.models.qa_model.list_vqa_models() to find all the available VQA models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Meta-Llama-3-8B-Instruct', 'gemma-2-9b-it', 'Qwen2-7B-Instruct']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tma.models.qa_model.text_qa_model import list_textqa_models\n",
    "\n",
    "# list all available models\n",
    "list_textqa_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `instructblip-flant5xl` as baseline model and `llavav1.5-7b` as model for comparing for showcasing, you can use other models you like or using multi-models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IMPORTANT] model cache is enabled, cache path: .cache/\n",
      "Loading Meta-Llama-3-8B-Instruct...\n",
      "HuggingFace meta-llama/Meta-Llama-3-8B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish loading Meta-Llama-3-8B-Instruct\n",
      "[IMPORTANT] model cache is enabled, cache path: .cache/\n",
      "Loading gemma-2-9b-it...\n",
      "HuggingFace google/gemma-2-9b-it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish loading gemma-2-9b-it\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from tma.models.qa_model.text_qa_model import TextQAModel\n",
    "from tma.models.qa_model import prompt\n",
    "import torch\n",
    "\n",
    "# single model\n",
    "baseline_model = TextQAModel(model_name='Meta-Llama-3-8B-Instruct', precision=torch.bfloat16, prompt_name = \"succinct_prompt\", prompt_func=prompt.succinct_prompt, cache_path = \".cache/\")\n",
    "model_to_compare = TextQAModel(model_name='gemma-2-9b-it', precision=torch.bfloat16, prompt_name = \"succinct_prompt\", prompt_func=prompt.succinct_prompt, cache_path = \".cache/\")\n",
    "\n",
    "\n",
    "# # multiple models\n",
    "# # Notice: If you have multiple GPUs, you can set the torch_device for each model to avoid running out of GPU memory.\n",
    "# model1 = ImageQAModel(model_name='llavav1.5-7b', torch_device=0, precision=torch.bfloat16, prompt_name = \"succinct_prompt\", prompt_func=prompt.succinct_prompt, cache_path = \".cache/\")\n",
    "# model2 = ImageQAModel(model_name='llavav1.5-13b', torch_device=1, precision=torch.bfloat16, prompt_name = \"succinct_prompt\", prompt_func=prompt.succinct_prompt, cache_path = \".cache/\")\n",
    " \n",
    "# baseline_models = [model1, model2]\n",
    "\n",
    "\n",
    "# model3 = ImageQAModel(model_name='qwenvl', torch_device=3, precision=torch.bfloat16, prompt_name = \"succinct_prompt\", prompt_func=prompt.succinct_prompt, cache_path = \".cache/\")\n",
    "# model4 = ImageQAModel(model_name='qwenvl-chat', torch_device=4, precision=torch.bfloat16, prompt_name = \"succinct_prompt\", prompt_func=prompt.succinct_prompt, cache_path = \".cache/\")\n",
    " \n",
    "# models_to_compare = [model3, model4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading model, we can start evaluating all the task plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is the perimeter (circumference) of a circle with radius 1.0?', 'answer': '6.283185307179586', 'task_plan': '{\"question_template\": \"What is the perimeter (circumference) of a circle with radius {param1}?\", \"radius\": \"1.0\", \"circumference\": \"6.283185307179586\", \"area\": \"3.141592653589793\"}', 'math_metadata': <tma.mathqa.math_metadata.MathTemplateMetaData object at 0x7fe6844e4090>}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'options'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# find the task plan that the model_to_compare performs better than the baseline_model above 30%\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m ground_truth_results \u001b[38;5;241m=\u001b[39m \u001b[43mtask_evaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_compare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgreater_than\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbaselines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mbaseline_model\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_to_compare\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_function_approximator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mmfs1/gscratch/krishna/bcaffee/Projects/TaskMeAnyMath/demo/query/../../tma/task_evaluator.py:755\u001b[0m, in \u001b[0;36mTaskEvaluator.model_compare\u001b[0;34m(self, x_indices, model, baselines, k, threshold, greater_than, ground_truth, function_approximator, fit_function_approximator)\u001b[0m\n\u001b[1;32m    744\u001b[0m \tresults, function_approximator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreshold_query(\n\u001b[1;32m    745\u001b[0m \t\tthreshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    746\u001b[0m \t\tx_indices\u001b[38;5;241m=\u001b[39mx_indices,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    752\u001b[0m \t\taggregate_func\u001b[38;5;241m=\u001b[39maggregate_func\n\u001b[1;32m    753\u001b[0m \t)\n\u001b[1;32m    754\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 755\u001b[0m \tresults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreshold_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mx_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mgreater_than\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgreater_than\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mground_truth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mground_truth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mfunction_approximator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction_approximator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mfit_function_approximator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43maggregate_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggregate_func\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m\t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    766\u001b[0m selection \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i, _ \u001b[38;5;129;01min\u001b[39;00m results]\n\u001b[1;32m    767\u001b[0m results \u001b[38;5;241m=\u001b[39m find_frequent_patterns(k, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_plan_df\u001b[38;5;241m.\u001b[39miloc[selection])\n",
      "File \u001b[0;32m/mmfs1/gscratch/krishna/bcaffee/Projects/TaskMeAnyMath/demo/query/../../tma/task_evaluator.py:467\u001b[0m, in \u001b[0;36mTaskEvaluator.threshold_query\u001b[0;34m(self, threshold, x_indices, model, greater_than, by, ground_truth, aggregate_func, function_approximator, fit_function_approximator)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mthreshold_query\u001b[39m(\n\u001b[1;32m    454\u001b[0m \t\t\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    455\u001b[0m \t\tthreshold: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    463\u001b[0m \t\tfit_function_approximator: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    464\u001b[0m ):\n\u001b[1;32m    465\u001b[0m \tindices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_groupby(by)\n\u001b[0;32m--> 467\u001b[0m \ty \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_ground_truth_for_all\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mx_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mfit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_function_approximator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mapproximator_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mregressor\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mfunction_approximator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction_approximator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mground_truth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mground_truth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43maggregate_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggregate_func\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m\t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m \taggregate_perf \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39mnanmean(y[i]) \u001b[38;5;28;01mfor\u001b[39;00m k, i \u001b[38;5;129;01min\u001b[39;00m indices])\n\u001b[1;32m    479\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m greater_than:\n",
      "File \u001b[0;32m/mmfs1/gscratch/krishna/bcaffee/Projects/TaskMeAnyMath/demo/query/../../tma/task_evaluator.py:199\u001b[0m, in \u001b[0;36mTaskEvaluator._get_ground_truth_for_all\u001b[0;34m(self, x_indices, fit, approximator_model, function_approximator, model, ground_truth, aggregate_func)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_ground_truth_for_all\u001b[39m(\n\u001b[1;32m    190\u001b[0m \t\t\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    191\u001b[0m \t\tx_indices,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    197\u001b[0m \t\taggregate_func: Union[\u001b[38;5;28mstr\u001b[39m, Callable] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    198\u001b[0m ):\n\u001b[0;32m--> 199\u001b[0m \tground_truth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_ground_truth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggregate_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m fit:\n\u001b[1;32m    202\u001b[0m \t\tfunction_approximator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m    203\u001b[0m \t\t\tx_indices\u001b[38;5;241m=\u001b[39mx_indices,\n\u001b[1;32m    204\u001b[0m \t\t\tapproximator_model\u001b[38;5;241m=\u001b[39mapproximator_model,\n\u001b[1;32m    205\u001b[0m \t\t\tfunction_approximator\u001b[38;5;241m=\u001b[39mfunction_approximator,\n\u001b[1;32m    206\u001b[0m \t\t\tground_truth\u001b[38;5;241m=\u001b[39mground_truth\n\u001b[1;32m    207\u001b[0m \t\t)\n",
      "File \u001b[0;32m/mmfs1/gscratch/krishna/bcaffee/Projects/TaskMeAnyMath/demo/query/../../tma/task_evaluator.py:183\u001b[0m, in \u001b[0;36mTaskEvaluator._get_ground_truth\u001b[0;34m(self, x_indices, model, ground_truth, aggregate_func)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m ground_truth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ground_truth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 183\u001b[0m \tground_truth \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate_many\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggregate_func\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m aggregate_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m \tground_truth \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([apply_aggregate_function(x, aggregate_func) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ground_truth])\n",
      "File \u001b[0;32m/mmfs1/gscratch/krishna/bcaffee/Projects/TaskMeAnyMath/demo/query/../../tma/task_evaluator.py:169\u001b[0m, in \u001b[0;36mTaskEvaluator._evaluate_many\u001b[0;34m(self, indices, model, aggregate_func)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_evaluate_many\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices, model: Union[Model, List[Model]], aggregate_func: Union[\u001b[38;5;28mstr\u001b[39m, Callable] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 169\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggregate_func\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEvaluating tasks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/mmfs1/gscratch/krishna/bcaffee/Projects/TaskMeAnyMath/demo/query/../../tma/task_evaluator.py:169\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_evaluate_many\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices, model: Union[Model, List[Model]], aggregate_func: Union[\u001b[38;5;28mstr\u001b[39m, Callable] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 169\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggregate_func\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(indices, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvaluating tasks\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n",
      "File \u001b[0;32m/mmfs1/gscratch/krishna/bcaffee/Projects/TaskMeAnyMath/demo/query/../../tma/task_evaluator.py:920\u001b[0m, in \u001b[0;36mQATaskEvaluator._evaluate_one\u001b[0;34m(self, plan_id, model, aggregate_func)\u001b[0m\n\u001b[1;32m    918\u001b[0m acc_m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverwrite_eval_cache \u001b[38;5;28;01melse\u001b[39;00m cache\u001b[38;5;241m.\u001b[39mget(key_str, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m acc_m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 920\u001b[0m \tacc_m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate_task_plan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask_plan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    921\u001b[0m \tcache\u001b[38;5;241m.\u001b[39mset(key_str, acc_m)\n\u001b[1;32m    922\u001b[0m acc\u001b[38;5;241m.\u001b[39mappend(acc_m)\n",
      "File \u001b[0;32m/mmfs1/gscratch/krishna/bcaffee/Projects/TaskMeAnyMath/demo/query/../../tma/task_evaluator.py:890\u001b[0m, in \u001b[0;36mQATaskEvaluator._evaluate_task_plan\u001b[0;34m(self, task_plan, model)\u001b[0m\n\u001b[1;32m    888\u001b[0m acc \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_instance_per_task):\n\u001b[0;32m--> 890\u001b[0m \ttask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask_plan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    891\u001b[0m \t\u001b[38;5;28;01mfor\u001b[39;00m ii \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_trials_per_instance):\n\u001b[1;32m    892\u001b[0m \t\tres \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmultiple_choice_qa(task[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_field], task[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m], task[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptions_trials\u001b[39m\u001b[38;5;124m'\u001b[39m][ii], task[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/mmfs1/gscratch/krishna/bcaffee/Projects/TaskMeAnyMath/demo/query/../../tma/task_evaluator.py:871\u001b[0m, in \u001b[0;36mQATaskEvaluator._generate_task\u001b[0;34m(self, task_plan, i)\u001b[0m\n\u001b[1;32m    869\u001b[0m \t\t\ttask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_generator\u001b[38;5;241m.\u001b[39mgenerate(task_plan, return_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    870\u001b[0m \t\t\t\u001b[38;5;28mprint\u001b[39m(task)\n\u001b[0;32m--> 871\u001b[0m \t\t\ttask[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptions_trials\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermutation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_trials_per_instance\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    872\u001b[0m \t\tcache\u001b[38;5;241m.\u001b[39mset(key_str, task)\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m task\n",
      "File \u001b[0;32m/mmfs1/gscratch/krishna/bcaffee/Projects/TaskMeAnyMath/demo/query/../../tma/task_evaluator.py:871\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    869\u001b[0m \t\t\ttask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_generator\u001b[38;5;241m.\u001b[39mgenerate(task_plan, return_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    870\u001b[0m \t\t\t\u001b[38;5;28mprint\u001b[39m(task)\n\u001b[0;32m--> 871\u001b[0m \t\t\ttask[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptions_trials\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrng\u001b[38;5;241m.\u001b[39mpermutation(\u001b[43mtask\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_trials_per_instance)]\n\u001b[1;32m    872\u001b[0m \t\tcache\u001b[38;5;241m.\u001b[39mset(key_str, task)\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m task\n",
      "\u001b[0;31mKeyError\u001b[0m: 'options'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "\n",
    "# find the task plan that the model_to_compare performs better than the baseline_model above 30%\n",
    "\n",
    "ground_truth_results = task_evaluator.model_compare(\n",
    "    x_indices=np.arange(len(df)),\n",
    "    greater_than=True,\n",
    "    threshold = 0.3,\n",
    "    baselines=[baseline_model],\n",
    "    model = model_to_compare,\n",
    "    fit_function_approximator=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(results):\n",
    "    pattern_stats = results[0]\n",
    "    # Determine the headers\n",
    "    headers = [\"Pattern\", \"Times\"]\n",
    "    \n",
    "    # Calculate the maximum length for formatting\n",
    "    max_pattern_length = max(len(str(plan[1])) for plan in pattern_stats)\n",
    "    \n",
    "    # Print the headers\n",
    "    print(f\"{headers[0]:<{max_pattern_length}} {headers[1]}\")\n",
    "    print(\"-\" * (max_pattern_length + len(headers[1]) + 1))\n",
    "    \n",
    "    # Iterate over the task plans and print each plan\n",
    "    for plan in pattern_stats:\n",
    "        task_id, attributes = plan\n",
    "        pattern = ', '.join([f\"{attr[0]}: {attr[1]}\" for attr in attributes])\n",
    "        print(f\"{pattern:<{max_pattern_length}} {task_id}\")\n",
    "        \n",
    "display_results(ground_truth_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply query approximation algorithms\n",
    "Query approximation algorithms means only evaluate model on a subset of tasks and use the result to approximate the performance on the whole task plans.\n",
    "\n",
    "We will use the `Fit` algorithm and `Active` algorithm to approximate the top k worst query, and compare the performance of these two methods with the ground truth. For each algorithm, we will give 500 budgets, which means the approximation algorithm can only evaluate 500 task plans.\n",
    "\n",
    "* In the `Fit` approach, we randomly select 500 task plans and fit the function approximator.\n",
    "* In the `Active` approach, we start with 200 task plans and then gradually add more task plans to the training set based on the function approximator's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are the functions to evaluate the approximation results with the ground truth\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "def compare_metric(gt, pred):\n",
    "\n",
    "    gt_selection = gt[1]\n",
    "    if len(gt_selection) == 0:\n",
    "        a = 1\n",
    "    pred_selection = pred[1]\n",
    "\n",
    "    # Determine the maximum index for array sizing\n",
    "    max_index = max(max(gt_selection, default=0), max(pred_selection, default=0))\n",
    "\n",
    "    # Initialize the labels based on the maximum index\n",
    "    gt_label = np.zeros(max_index + 1)\n",
    "    pred_label = np.zeros(max_index + 1)\n",
    "\n",
    "    for k in gt_selection:\n",
    "        gt_label[k] = 1\n",
    "\n",
    "    for k in pred_selection:\n",
    "        pred_label[k] = 1\n",
    "\n",
    "    f1 = f1_score(gt_label, pred_label) * 100\n",
    "    acc = accuracy_score(gt_label, pred_label) * 100\n",
    "    precision = precision_score(gt_label, pred_label) * 100\n",
    "    recall = recall_score(gt_label, pred_label) * 100\n",
    "\n",
    "    return precision, recall, f1, acc\n",
    "\n",
    "def print_metrics(precision, recall, f1, acc):\n",
    "    print(f\"{'Metric':<15} {'Value':<10}\")\n",
    "    print(\"-\" * 25)\n",
    "    print(f\"{'Precision:':<15} {precision:.2f}%\")\n",
    "    print(f\"{'Recall:':<15} {recall:.2f}%\")\n",
    "    print(f\"{'F1 Score:':<15} {f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use \"Fit\" approximation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "budget = 500\n",
    "np.random.seed(42)\n",
    "perm = np.random.permutation(len(df))\n",
    "x_indices = perm[:budget]\n",
    "\n",
    "fit_results = task_evaluator.model_compare(\n",
    "    x_indices=x_indices,\n",
    "    greater_than=True,\n",
    "    threshold = 0.2,\n",
    "    baselines=[baseline_model],\n",
    "    model = model_to_compare,\n",
    "    fit_function_approximator=True\n",
    ")\n",
    "precision, recall, f1, acc = compare_metric(ground_truth_results, fit_results)\n",
    "print_metrics(precision, recall, f1, acc)\n",
    "display_results(fit_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use \"Active\" approximation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_budget=200\n",
    "active_results = task_evaluator.active_model_compare(\n",
    "    k=10,\n",
    "    warmup_budget=warmup_budget,\n",
    "    budget=budget-warmup_budget,\n",
    "    greater_than=True,\n",
    "    threshold = 0.2,\n",
    "    baselines=[baseline_model],\n",
    "    model = model_to_compare,\n",
    ")\n",
    "\n",
    "precision, recall, f1, acc = compare_metric(ground_truth_results, active_results)\n",
    "print_metrics(precision, recall, f1, acc)\n",
    "display_results(active_results[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
